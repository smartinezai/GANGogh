{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tAI9KK9wiNaN"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h0m4HVIjw4N",
        "outputId": "1533fd82-cd28-4dc2-9eeb-00a8d4b41a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorch-GAN'...\n",
            "remote: Enumerating objects: 1283, done.\u001b[K\n",
            "remote: Total 1283 (delta 0), reused 0 (delta 0), pack-reused 1283\u001b[K\n",
            "Receiving objects: 100% (1283/1283), 68.04 MiB | 35.51 MiB/s, done.\n",
            "Resolving deltas: 100% (751/751), done.\n",
            "/content/PyTorch-GAN\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.11.1+cu111)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.18.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->-r requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 8)) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 8)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 8)) (2.6.3)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/eriklindernoren/PyTorch-GAN\n",
        "%cd PyTorch-GAN/\n",
        "%pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWt3F_9FkGT4",
        "outputId": "37442318-c09b-4890-f1a8-9e5166ff5e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/PyTorch-GAN/implementations/lsgan\n"
          ]
        }
      ],
      "source": [
        "%cd implementations/lsgan/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g1xqtchKkRU5",
        "outputId": "92a7c565-1c00-4497-a17b-b8d705dd887d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/PyTorch-GAN/implementations/lsgan'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note to self: Remember to replace lsgan file with own tweaks for it to work with our dataset"
      ],
      "metadata": {
        "id": "1SvBqZaZfmT9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYNGkcHH2aWr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpymAq5hkS3k",
        "outputId": "e6ec22f9-7bc5-474c-e504-9a3ed2bcff66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=3, img_size=128, latent_dim=100, lr=0.0001, n_cpu=4, n_epochs=10, sample_interval=1000)\n",
            "[Epoch 0/10] [Batch 0/68] [D loss: 0.510512] [G loss: 1.020816]\n",
            "[Epoch 0/10] [Batch 1/68] [D loss: 0.509480] [G loss: 1.019170]\n",
            "[Epoch 0/10] [Batch 2/68] [D loss: 0.508882] [G loss: 1.017226]\n",
            "[Epoch 0/10] [Batch 3/68] [D loss: 0.507629] [G loss: 1.015386]\n",
            "[Epoch 0/10] [Batch 4/68] [D loss: 0.506474] [G loss: 1.012899]\n",
            "[Epoch 0/10] [Batch 5/68] [D loss: 0.504901] [G loss: 1.009777]\n",
            "[Epoch 0/10] [Batch 6/68] [D loss: 0.503100] [G loss: 1.006536]\n",
            "[Epoch 0/10] [Batch 7/68] [D loss: 0.501019] [G loss: 1.002618]\n",
            "[Epoch 0/10] [Batch 8/68] [D loss: 0.498826] [G loss: 0.998011]\n",
            "[Epoch 0/10] [Batch 9/68] [D loss: 0.495621] [G loss: 0.992132]\n",
            "[Epoch 0/10] [Batch 10/68] [D loss: 0.492676] [G loss: 0.986125]\n",
            "[Epoch 0/10] [Batch 11/68] [D loss: 0.489182] [G loss: 0.978892]\n",
            "[Epoch 0/10] [Batch 12/68] [D loss: 0.485229] [G loss: 0.970892]\n",
            "[Epoch 0/10] [Batch 13/68] [D loss: 0.480502] [G loss: 0.962128]\n",
            "[Epoch 0/10] [Batch 14/68] [D loss: 0.476127] [G loss: 0.951555]\n",
            "[Epoch 0/10] [Batch 15/68] [D loss: 0.470208] [G loss: 0.941416]\n",
            "[Epoch 0/10] [Batch 16/68] [D loss: 0.464931] [G loss: 0.929686]\n",
            "[Epoch 0/10] [Batch 17/68] [D loss: 0.459118] [G loss: 0.918038]\n",
            "[Epoch 0/10] [Batch 18/68] [D loss: 0.452545] [G loss: 0.904931]\n",
            "[Epoch 0/10] [Batch 19/68] [D loss: 0.446041] [G loss: 0.891469]\n",
            "[Epoch 0/10] [Batch 20/68] [D loss: 0.439311] [G loss: 0.876314]\n",
            "[Epoch 0/10] [Batch 21/68] [D loss: 0.431780] [G loss: 0.860451]\n",
            "[Epoch 0/10] [Batch 22/68] [D loss: 0.423898] [G loss: 0.844177]\n",
            "[Epoch 0/10] [Batch 23/68] [D loss: 0.416517] [G loss: 0.829093]\n",
            "[Epoch 0/10] [Batch 24/68] [D loss: 0.409985] [G loss: 0.811744]\n",
            "[Epoch 0/10] [Batch 25/68] [D loss: 0.399976] [G loss: 0.793510]\n",
            "[Epoch 0/10] [Batch 26/68] [D loss: 0.391991] [G loss: 0.776022]\n",
            "[Epoch 0/10] [Batch 27/68] [D loss: 0.382004] [G loss: 0.756450]\n",
            "[Epoch 0/10] [Batch 28/68] [D loss: 0.374386] [G loss: 0.738421]\n",
            "[Epoch 0/10] [Batch 29/68] [D loss: 0.368654] [G loss: 0.715880]\n",
            "[Epoch 0/10] [Batch 30/68] [D loss: 0.356534] [G loss: 0.694354]\n",
            "[Epoch 0/10] [Batch 31/68] [D loss: 0.349242] [G loss: 0.673375]\n",
            "[Epoch 0/10] [Batch 32/68] [D loss: 0.342058] [G loss: 0.652565]\n",
            "[Epoch 0/10] [Batch 33/68] [D loss: 0.331118] [G loss: 0.629061]\n",
            "[Epoch 0/10] [Batch 34/68] [D loss: 0.326384] [G loss: 0.601640]\n",
            "[Epoch 0/10] [Batch 35/68] [D loss: 0.324227] [G loss: 0.581147]\n",
            "[Epoch 0/10] [Batch 36/68] [D loss: 0.311577] [G loss: 0.558276]\n",
            "[Epoch 0/10] [Batch 37/68] [D loss: 0.306250] [G loss: 0.518157]\n",
            "[Epoch 0/10] [Batch 38/68] [D loss: 0.292071] [G loss: 0.492849]\n",
            "[Epoch 0/10] [Batch 39/68] [D loss: 0.303944] [G loss: 0.463761]\n",
            "[Epoch 0/10] [Batch 40/68] [D loss: 0.290433] [G loss: 0.441014]\n",
            "[Epoch 0/10] [Batch 41/68] [D loss: 0.297546] [G loss: 0.420864]\n",
            "[Epoch 0/10] [Batch 42/68] [D loss: 0.298737] [G loss: 0.406148]\n",
            "[Epoch 0/10] [Batch 43/68] [D loss: 0.282762] [G loss: 0.411205]\n",
            "[Epoch 0/10] [Batch 44/68] [D loss: 0.289500] [G loss: 0.388525]\n",
            "[Epoch 0/10] [Batch 45/68] [D loss: 0.279071] [G loss: 0.361340]\n",
            "[Epoch 0/10] [Batch 46/68] [D loss: 0.278927] [G loss: 0.364018]\n",
            "[Epoch 0/10] [Batch 47/68] [D loss: 0.281062] [G loss: 0.362953]\n",
            "[Epoch 0/10] [Batch 48/68] [D loss: 0.275613] [G loss: 0.340560]\n",
            "[Epoch 0/10] [Batch 49/68] [D loss: 0.279233] [G loss: 0.354104]\n",
            "[Epoch 0/10] [Batch 50/68] [D loss: 0.268429] [G loss: 0.331725]\n",
            "[Epoch 0/10] [Batch 51/68] [D loss: 0.269389] [G loss: 0.337785]\n",
            "[Epoch 0/10] [Batch 52/68] [D loss: 0.269514] [G loss: 0.333980]\n",
            "[Epoch 0/10] [Batch 53/68] [D loss: 0.261542] [G loss: 0.332753]\n",
            "[Epoch 0/10] [Batch 54/68] [D loss: 0.266507] [G loss: 0.326967]\n",
            "[Epoch 0/10] [Batch 55/68] [D loss: 0.267002] [G loss: 0.317413]\n",
            "[Epoch 0/10] [Batch 56/68] [D loss: 0.263647] [G loss: 0.315351]\n",
            "[Epoch 0/10] [Batch 57/68] [D loss: 0.262438] [G loss: 0.322502]\n",
            "[Epoch 0/10] [Batch 58/68] [D loss: 0.258096] [G loss: 0.316539]\n",
            "[Epoch 0/10] [Batch 59/68] [D loss: 0.257303] [G loss: 0.320065]\n",
            "[Epoch 0/10] [Batch 60/68] [D loss: 0.255692] [G loss: 0.312099]\n",
            "[Epoch 0/10] [Batch 61/68] [D loss: 0.252766] [G loss: 0.314472]\n",
            "[Epoch 0/10] [Batch 62/68] [D loss: 0.249322] [G loss: 0.311343]\n",
            "[Epoch 0/10] [Batch 63/68] [D loss: 0.250765] [G loss: 0.311306]\n",
            "[Epoch 0/10] [Batch 64/68] [D loss: 0.249347] [G loss: 0.303706]\n",
            "[Epoch 0/10] [Batch 65/68] [D loss: 0.248624] [G loss: 0.306137]\n",
            "[Epoch 0/10] [Batch 66/68] [D loss: 0.246251] [G loss: 0.299776]\n",
            "[Epoch 0/10] [Batch 67/68] [D loss: 0.259746] [G loss: 0.286873]\n",
            "[Epoch 1/10] [Batch 0/68] [D loss: 0.245521] [G loss: 0.301837]\n",
            "[Epoch 1/10] [Batch 1/68] [D loss: 0.241563] [G loss: 0.305695]\n",
            "[Epoch 1/10] [Batch 2/68] [D loss: 0.239974] [G loss: 0.306314]\n",
            "[Epoch 1/10] [Batch 3/68] [D loss: 0.238497] [G loss: 0.301420]\n",
            "[Epoch 1/10] [Batch 4/68] [D loss: 0.245246] [G loss: 0.300890]\n",
            "[Epoch 1/10] [Batch 5/68] [D loss: 0.240216] [G loss: 0.297164]\n",
            "[Epoch 1/10] [Batch 6/68] [D loss: 0.232160] [G loss: 0.311206]\n",
            "[Epoch 1/10] [Batch 7/68] [D loss: 0.242809] [G loss: 0.314159]\n",
            "[Epoch 1/10] [Batch 8/68] [D loss: 0.234612] [G loss: 0.323827]\n",
            "[Epoch 1/10] [Batch 9/68] [D loss: 0.244528] [G loss: 0.320335]\n",
            "[Epoch 1/10] [Batch 10/68] [D loss: 0.244451] [G loss: 0.300932]\n",
            "[Epoch 1/10] [Batch 11/68] [D loss: 0.235113] [G loss: 0.300473]\n",
            "[Epoch 1/10] [Batch 12/68] [D loss: 0.250267] [G loss: 0.287208]\n",
            "[Epoch 1/10] [Batch 13/68] [D loss: 0.251156] [G loss: 0.271610]\n",
            "[Epoch 1/10] [Batch 14/68] [D loss: 0.245236] [G loss: 0.281286]\n",
            "[Epoch 1/10] [Batch 15/68] [D loss: 0.243154] [G loss: 0.274297]\n",
            "[Epoch 1/10] [Batch 16/68] [D loss: 0.253446] [G loss: 0.262358]\n",
            "[Epoch 1/10] [Batch 17/68] [D loss: 0.248203] [G loss: 0.268660]\n",
            "[Epoch 1/10] [Batch 18/68] [D loss: 0.246563] [G loss: 0.263691]\n",
            "[Epoch 1/10] [Batch 19/68] [D loss: 0.238512] [G loss: 0.261622]\n",
            "[Epoch 1/10] [Batch 20/68] [D loss: 0.244744] [G loss: 0.255992]\n",
            "[Epoch 1/10] [Batch 21/68] [D loss: 0.248489] [G loss: 0.261636]\n",
            "[Epoch 1/10] [Batch 22/68] [D loss: 0.245071] [G loss: 0.268593]\n",
            "[Epoch 1/10] [Batch 23/68] [D loss: 0.248994] [G loss: 0.255601]\n",
            "[Epoch 1/10] [Batch 24/68] [D loss: 0.247172] [G loss: 0.259374]\n",
            "[Epoch 1/10] [Batch 25/68] [D loss: 0.251344] [G loss: 0.259809]\n",
            "[Epoch 1/10] [Batch 26/68] [D loss: 0.256185] [G loss: 0.244575]\n",
            "[Epoch 1/10] [Batch 27/68] [D loss: 0.247003] [G loss: 0.247991]\n",
            "[Epoch 1/10] [Batch 28/68] [D loss: 0.252440] [G loss: 0.237716]\n",
            "[Epoch 1/10] [Batch 29/68] [D loss: 0.248815] [G loss: 0.250214]\n",
            "[Epoch 1/10] [Batch 30/68] [D loss: 0.256100] [G loss: 0.245108]\n",
            "[Epoch 1/10] [Batch 31/68] [D loss: 0.256491] [G loss: 0.241997]\n",
            "[Epoch 1/10] [Batch 32/68] [D loss: 0.252499] [G loss: 0.242629]\n",
            "[Epoch 1/10] [Batch 33/68] [D loss: 0.256569] [G loss: 0.248323]\n",
            "[Epoch 1/10] [Batch 34/68] [D loss: 0.257155] [G loss: 0.244020]\n",
            "[Epoch 1/10] [Batch 35/68] [D loss: 0.253147] [G loss: 0.249034]\n",
            "[Epoch 1/10] [Batch 36/68] [D loss: 0.251960] [G loss: 0.243929]\n",
            "[Epoch 1/10] [Batch 37/68] [D loss: 0.252284] [G loss: 0.256684]\n",
            "[Epoch 1/10] [Batch 38/68] [D loss: 0.254265] [G loss: 0.250255]\n",
            "[Epoch 1/10] [Batch 39/68] [D loss: 0.253259] [G loss: 0.248556]\n",
            "[Epoch 1/10] [Batch 40/68] [D loss: 0.244513] [G loss: 0.250147]\n",
            "[Epoch 1/10] [Batch 41/68] [D loss: 0.249579] [G loss: 0.251865]\n",
            "[Epoch 1/10] [Batch 42/68] [D loss: 0.247250] [G loss: 0.256932]\n",
            "[Epoch 1/10] [Batch 43/68] [D loss: 0.248481] [G loss: 0.259637]\n",
            "[Epoch 1/10] [Batch 44/68] [D loss: 0.242567] [G loss: 0.252575]\n",
            "[Epoch 1/10] [Batch 45/68] [D loss: 0.247969] [G loss: 0.256993]\n",
            "[Epoch 1/10] [Batch 46/68] [D loss: 0.237925] [G loss: 0.254927]\n",
            "[Epoch 1/10] [Batch 47/68] [D loss: 0.244315] [G loss: 0.252534]\n",
            "[Epoch 1/10] [Batch 48/68] [D loss: 0.242985] [G loss: 0.256101]\n",
            "[Epoch 1/10] [Batch 49/68] [D loss: 0.239646] [G loss: 0.261185]\n",
            "[Epoch 1/10] [Batch 50/68] [D loss: 0.240619] [G loss: 0.254591]\n",
            "[Epoch 1/10] [Batch 51/68] [D loss: 0.237730] [G loss: 0.256769]\n",
            "[Epoch 1/10] [Batch 52/68] [D loss: 0.245824] [G loss: 0.252076]\n",
            "[Epoch 1/10] [Batch 53/68] [D loss: 0.239456] [G loss: 0.260232]\n",
            "[Epoch 1/10] [Batch 54/68] [D loss: 0.241747] [G loss: 0.251235]\n",
            "[Epoch 1/10] [Batch 55/68] [D loss: 0.245214] [G loss: 0.252520]\n",
            "[Epoch 1/10] [Batch 56/68] [D loss: 0.244707] [G loss: 0.253102]\n",
            "[Epoch 1/10] [Batch 57/68] [D loss: 0.229249] [G loss: 0.263602]\n",
            "[Epoch 1/10] [Batch 58/68] [D loss: 0.243568] [G loss: 0.250355]\n",
            "[Epoch 1/10] [Batch 59/68] [D loss: 0.241115] [G loss: 0.257220]\n",
            "[Epoch 1/10] [Batch 60/68] [D loss: 0.241227] [G loss: 0.247545]\n",
            "[Epoch 1/10] [Batch 61/68] [D loss: 0.239720] [G loss: 0.264513]\n",
            "[Epoch 1/10] [Batch 62/68] [D loss: 0.238617] [G loss: 0.277873]\n",
            "[Epoch 1/10] [Batch 63/68] [D loss: 0.241073] [G loss: 0.262285]\n",
            "[Epoch 1/10] [Batch 64/68] [D loss: 0.229648] [G loss: 0.271872]\n",
            "[Epoch 1/10] [Batch 65/68] [D loss: 0.239972] [G loss: 0.261020]\n",
            "[Epoch 1/10] [Batch 66/68] [D loss: 0.242439] [G loss: 0.258260]\n",
            "[Epoch 1/10] [Batch 67/68] [D loss: 0.252962] [G loss: 0.254611]\n",
            "[Epoch 2/10] [Batch 0/68] [D loss: 0.236955] [G loss: 0.269843]\n",
            "[Epoch 2/10] [Batch 1/68] [D loss: 0.243762] [G loss: 0.274623]\n",
            "[Epoch 2/10] [Batch 2/68] [D loss: 0.234587] [G loss: 0.270252]\n",
            "[Epoch 2/10] [Batch 3/68] [D loss: 0.245278] [G loss: 0.262333]\n",
            "[Epoch 2/10] [Batch 4/68] [D loss: 0.248285] [G loss: 0.259749]\n",
            "[Epoch 2/10] [Batch 5/68] [D loss: 0.240188] [G loss: 0.264665]\n",
            "[Epoch 2/10] [Batch 6/68] [D loss: 0.243732] [G loss: 0.266804]\n",
            "[Epoch 2/10] [Batch 7/68] [D loss: 0.238958] [G loss: 0.274843]\n",
            "[Epoch 2/10] [Batch 8/68] [D loss: 0.243136] [G loss: 0.266760]\n",
            "[Epoch 2/10] [Batch 9/68] [D loss: 0.242741] [G loss: 0.261892]\n",
            "[Epoch 2/10] [Batch 10/68] [D loss: 0.245682] [G loss: 0.264918]\n",
            "[Epoch 2/10] [Batch 11/68] [D loss: 0.239409] [G loss: 0.270210]\n",
            "[Epoch 2/10] [Batch 12/68] [D loss: 0.240910] [G loss: 0.259714]\n",
            "[Epoch 2/10] [Batch 13/68] [D loss: 0.238822] [G loss: 0.263145]\n",
            "[Epoch 2/10] [Batch 14/68] [D loss: 0.241281] [G loss: 0.268794]\n",
            "[Epoch 2/10] [Batch 15/68] [D loss: 0.243682] [G loss: 0.264329]\n",
            "[Epoch 2/10] [Batch 16/68] [D loss: 0.242695] [G loss: 0.267425]\n",
            "[Epoch 2/10] [Batch 17/68] [D loss: 0.239404] [G loss: 0.284994]\n",
            "[Epoch 2/10] [Batch 18/68] [D loss: 0.242200] [G loss: 0.294280]\n",
            "[Epoch 2/10] [Batch 19/68] [D loss: 0.247065] [G loss: 0.270454]\n",
            "[Epoch 2/10] [Batch 20/68] [D loss: 0.253414] [G loss: 0.260526]\n",
            "[Epoch 2/10] [Batch 21/68] [D loss: 0.238466] [G loss: 0.276174]\n",
            "[Epoch 2/10] [Batch 22/68] [D loss: 0.245895] [G loss: 0.272874]\n",
            "[Epoch 2/10] [Batch 23/68] [D loss: 0.241498] [G loss: 0.253008]\n",
            "[Epoch 2/10] [Batch 24/68] [D loss: 0.247059] [G loss: 0.252091]\n",
            "[Epoch 2/10] [Batch 25/68] [D loss: 0.252355] [G loss: 0.259630]\n",
            "[Epoch 2/10] [Batch 26/68] [D loss: 0.249419] [G loss: 0.257033]\n",
            "[Epoch 2/10] [Batch 27/68] [D loss: 0.256524] [G loss: 0.253331]\n",
            "[Epoch 2/10] [Batch 28/68] [D loss: 0.250347] [G loss: 0.248716]\n",
            "[Epoch 2/10] [Batch 29/68] [D loss: 0.255060] [G loss: 0.240895]\n",
            "[Epoch 2/10] [Batch 30/68] [D loss: 0.248729] [G loss: 0.250538]\n",
            "[Epoch 2/10] [Batch 31/68] [D loss: 0.253298] [G loss: 0.250642]\n",
            "[Epoch 2/10] [Batch 32/68] [D loss: 0.249302] [G loss: 0.246751]\n",
            "[Epoch 2/10] [Batch 33/68] [D loss: 0.240705] [G loss: 0.248309]\n",
            "[Epoch 2/10] [Batch 34/68] [D loss: 0.243780] [G loss: 0.240061]\n",
            "[Epoch 2/10] [Batch 35/68] [D loss: 0.247898] [G loss: 0.245111]\n",
            "[Epoch 2/10] [Batch 36/68] [D loss: 0.245808] [G loss: 0.245059]\n",
            "[Epoch 2/10] [Batch 37/68] [D loss: 0.246238] [G loss: 0.242056]\n",
            "[Epoch 2/10] [Batch 38/68] [D loss: 0.240280] [G loss: 0.235773]\n",
            "[Epoch 2/10] [Batch 39/68] [D loss: 0.245758] [G loss: 0.242110]\n",
            "[Epoch 2/10] [Batch 40/68] [D loss: 0.245991] [G loss: 0.247677]\n",
            "[Epoch 2/10] [Batch 41/68] [D loss: 0.237518] [G loss: 0.237004]\n",
            "[Epoch 2/10] [Batch 42/68] [D loss: 0.249167] [G loss: 0.243205]\n",
            "[Epoch 2/10] [Batch 43/68] [D loss: 0.249807] [G loss: 0.235366]\n",
            "[Epoch 2/10] [Batch 44/68] [D loss: 0.235468] [G loss: 0.243313]\n",
            "[Epoch 2/10] [Batch 45/68] [D loss: 0.247443] [G loss: 0.255534]\n",
            "[Epoch 2/10] [Batch 46/68] [D loss: 0.250670] [G loss: 0.267095]\n",
            "[Epoch 2/10] [Batch 47/68] [D loss: 0.252502] [G loss: 0.245581]\n",
            "[Epoch 2/10] [Batch 48/68] [D loss: 0.253790] [G loss: 0.261583]\n",
            "[Epoch 2/10] [Batch 49/68] [D loss: 0.246341] [G loss: 0.264774]\n",
            "[Epoch 2/10] [Batch 50/68] [D loss: 0.263044] [G loss: 0.253125]\n",
            "[Epoch 2/10] [Batch 51/68] [D loss: 0.247734] [G loss: 0.265225]\n",
            "[Epoch 2/10] [Batch 52/68] [D loss: 0.250076] [G loss: 0.282611]\n",
            "[Epoch 2/10] [Batch 53/68] [D loss: 0.248157] [G loss: 0.260414]\n",
            "[Epoch 2/10] [Batch 54/68] [D loss: 0.250135] [G loss: 0.284900]\n",
            "[Epoch 2/10] [Batch 55/68] [D loss: 0.240601] [G loss: 0.290597]\n",
            "[Epoch 2/10] [Batch 56/68] [D loss: 0.250558] [G loss: 0.265101]\n",
            "[Epoch 2/10] [Batch 57/68] [D loss: 0.250705] [G loss: 0.282510]\n",
            "[Epoch 2/10] [Batch 58/68] [D loss: 0.240403] [G loss: 0.274718]\n",
            "[Epoch 2/10] [Batch 59/68] [D loss: 0.243607] [G loss: 0.280413]\n",
            "[Epoch 2/10] [Batch 60/68] [D loss: 0.244994] [G loss: 0.280792]\n",
            "[Epoch 2/10] [Batch 61/68] [D loss: 0.249677] [G loss: 0.289099]\n",
            "[Epoch 2/10] [Batch 62/68] [D loss: 0.251102] [G loss: 0.284546]\n",
            "[Epoch 2/10] [Batch 63/68] [D loss: 0.245824] [G loss: 0.282321]\n",
            "[Epoch 2/10] [Batch 64/68] [D loss: 0.244622] [G loss: 0.271493]\n",
            "[Epoch 2/10] [Batch 65/68] [D loss: 0.250025] [G loss: 0.273030]\n",
            "[Epoch 2/10] [Batch 66/68] [D loss: 0.248675] [G loss: 0.270532]\n",
            "[Epoch 2/10] [Batch 67/68] [D loss: 0.251926] [G loss: 0.281069]\n",
            "[Epoch 3/10] [Batch 0/68] [D loss: 0.243303] [G loss: 0.257599]\n",
            "[Epoch 3/10] [Batch 1/68] [D loss: 0.243717] [G loss: 0.250733]\n",
            "[Epoch 3/10] [Batch 2/68] [D loss: 0.251527] [G loss: 0.241254]\n",
            "[Epoch 3/10] [Batch 3/68] [D loss: 0.252908] [G loss: 0.262845]\n",
            "[Epoch 3/10] [Batch 4/68] [D loss: 0.253261] [G loss: 0.252941]\n",
            "[Epoch 3/10] [Batch 5/68] [D loss: 0.250850] [G loss: 0.250782]\n",
            "[Epoch 3/10] [Batch 6/68] [D loss: 0.244712] [G loss: 0.245946]\n",
            "[Epoch 3/10] [Batch 7/68] [D loss: 0.254386] [G loss: 0.244041]\n",
            "[Epoch 3/10] [Batch 8/68] [D loss: 0.251340] [G loss: 0.244454]\n",
            "[Epoch 3/10] [Batch 9/68] [D loss: 0.252279] [G loss: 0.244258]\n",
            "[Epoch 3/10] [Batch 10/68] [D loss: 0.242190] [G loss: 0.241510]\n",
            "[Epoch 3/10] [Batch 11/68] [D loss: 0.247888] [G loss: 0.240668]\n",
            "[Epoch 3/10] [Batch 12/68] [D loss: 0.247008] [G loss: 0.237149]\n",
            "[Epoch 3/10] [Batch 13/68] [D loss: 0.248322] [G loss: 0.230250]\n",
            "[Epoch 3/10] [Batch 14/68] [D loss: 0.248799] [G loss: 0.239422]\n",
            "[Epoch 3/10] [Batch 15/68] [D loss: 0.248128] [G loss: 0.238609]\n",
            "[Epoch 3/10] [Batch 16/68] [D loss: 0.248497] [G loss: 0.249935]\n",
            "[Epoch 3/10] [Batch 17/68] [D loss: 0.255150] [G loss: 0.248035]\n",
            "[Epoch 3/10] [Batch 18/68] [D loss: 0.245110] [G loss: 0.248983]\n",
            "[Epoch 3/10] [Batch 19/68] [D loss: 0.253613] [G loss: 0.243908]\n",
            "[Epoch 3/10] [Batch 20/68] [D loss: 0.253595] [G loss: 0.248033]\n",
            "[Epoch 3/10] [Batch 21/68] [D loss: 0.252480] [G loss: 0.250971]\n",
            "[Epoch 3/10] [Batch 22/68] [D loss: 0.264104] [G loss: 0.251051]\n",
            "[Epoch 3/10] [Batch 23/68] [D loss: 0.250399] [G loss: 0.267038]\n",
            "[Epoch 3/10] [Batch 24/68] [D loss: 0.256766] [G loss: 0.261255]\n",
            "[Epoch 3/10] [Batch 25/68] [D loss: 0.255542] [G loss: 0.269326]\n",
            "[Epoch 3/10] [Batch 26/68] [D loss: 0.247547] [G loss: 0.264506]\n",
            "[Epoch 3/10] [Batch 27/68] [D loss: 0.257063] [G loss: 0.246035]\n",
            "[Epoch 3/10] [Batch 28/68] [D loss: 0.250186] [G loss: 0.263469]\n",
            "[Epoch 3/10] [Batch 29/68] [D loss: 0.260888] [G loss: 0.260890]\n",
            "[Epoch 3/10] [Batch 30/68] [D loss: 0.253016] [G loss: 0.256560]\n",
            "[Epoch 3/10] [Batch 31/68] [D loss: 0.255516] [G loss: 0.274956]\n",
            "[Epoch 3/10] [Batch 32/68] [D loss: 0.253175] [G loss: 0.261298]\n",
            "[Epoch 3/10] [Batch 33/68] [D loss: 0.250387] [G loss: 0.268784]\n",
            "[Epoch 3/10] [Batch 34/68] [D loss: 0.257347] [G loss: 0.269540]\n",
            "[Epoch 3/10] [Batch 35/68] [D loss: 0.252423] [G loss: 0.271502]\n",
            "[Epoch 3/10] [Batch 36/68] [D loss: 0.250743] [G loss: 0.261211]\n",
            "[Epoch 3/10] [Batch 37/68] [D loss: 0.249593] [G loss: 0.259971]\n",
            "[Epoch 3/10] [Batch 38/68] [D loss: 0.251389] [G loss: 0.263099]\n",
            "[Epoch 3/10] [Batch 39/68] [D loss: 0.248199] [G loss: 0.253859]\n",
            "[Epoch 3/10] [Batch 40/68] [D loss: 0.252677] [G loss: 0.254427]\n",
            "[Epoch 3/10] [Batch 41/68] [D loss: 0.248176] [G loss: 0.254705]\n",
            "[Epoch 3/10] [Batch 42/68] [D loss: 0.248536] [G loss: 0.253958]\n",
            "[Epoch 3/10] [Batch 43/68] [D loss: 0.254731] [G loss: 0.248906]\n",
            "[Epoch 3/10] [Batch 44/68] [D loss: 0.251305] [G loss: 0.252972]\n",
            "[Epoch 3/10] [Batch 45/68] [D loss: 0.241873] [G loss: 0.240990]\n",
            "[Epoch 3/10] [Batch 46/68] [D loss: 0.251902] [G loss: 0.247592]\n",
            "[Epoch 3/10] [Batch 47/68] [D loss: 0.253672] [G loss: 0.252880]\n"
          ]
        }
      ],
      "source": [
        "!python3 lsgan.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHawfnwVDOSd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL0hwNhe5Iy_"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MRAFIeba5WM2",
        "outputId": "680e4ce5-4cce-4672-9bc1-3f6c9180574e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/PyTorch-GAN/implementations/lsgan'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCfNHNGN1kYz"
      },
      "outputs": [],
      "source": [
        "nz = 100\n",
        "noise = torch.randn(4, nz, 1, 1,device=gan1.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umzh7FIh025r"
      },
      "outputs": [],
      "source": [
        "def generate(self,z):\n",
        "        #self.generator.eval()\n",
        "        return self.generator(z)\n",
        "\n",
        "    def resizeTensor(self,tensor,size): #resizes the images of an tensor without a record in calc-graph!\n",
        "        with torch.no_grad():\n",
        "            result = torchvision.transforms.Resize(size)(tensor)\n",
        "        return result\n",
        "\n",
        "    def plot_training_images(self):\n",
        "        # Plot some training images\n",
        "        batch = next(iter(self.dataloader))\n",
        "        plt.figure(figsize=(8,8))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Training Images\")\n",
        "        plt.imshow(np.transpose(vutils.make_grid(batch[0].to(self.device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "        plt.show()\n",
        "\n",
        "    def plot_generated_images(self):\n",
        "        noise = torch.randn(64, nz, 1, 1,device=self.device)\n",
        "        images = self.generate(noise)\n",
        "        #print(images.size())\n",
        "        plt.figure(figsize=(8,8))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Generated Images\")\n",
        "        plt.imshow(np.transpose(vutils.make_grid(images.to(self.device), padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "        plt.show()\n",
        "\n",
        "    def plot_generated_image(self):\n",
        "        noise = torch.randn(1, nz, 1, 1, device=self.device)\n",
        "        image = self.generate(noise)[0]\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Generated image\")\n",
        "        plt.imshow(np.transpose(vutils.make_grid(image,normalize=True).cpu().detach().numpy(),(1,2,0)))\n",
        "        plt.show()\n",
        "\n",
        "    def save(self,path=\"\",saveDiscriminator=False):\n",
        "        torch.save(self.generator, path+\"generator_save\")\n",
        "        if saveDiscriminator:\n",
        "            torch.save(self.discriminator, path+\"discriminator_save\")\n",
        "\n",
        "    def load(self,path=\"\",loadDiscriminator=False):\n",
        "        self.generator = torch.load(path+\"generator_save\")\n",
        "        #self.generator.eval()\n",
        "        #self.generator.eval()\n",
        "        if loadDiscriminator:\n",
        "            self.discriminator = torch.load(path+\"discriminator_save\")\n",
        "            self.discriminator.eval()\n",
        "\n",
        "    def plot_training_loss(self,round_str=\"\",_num_epochs=num_epochs):\n",
        "        #print(self.G_losses)\n",
        "        if len(self.G_losses) > 0:\n",
        "            plt.figure(figsize=(10,5))\n",
        "            plt.title(\"Generator and Discriminator Loss During Training \"+round_str)\n",
        "            if self.type == 'WGAN' or 'ProWGAN':\n",
        "                plt.plot(range(0,len(self.D_losses)),self.D_losses, label=\"D\", color=\"blue\")\n",
        "                plt.plot([n_critic*i for i in range(len(self.G_losses))],self.G_losses, label=\"G\", color=\"red\")\n",
        "            else:\n",
        "                plt.plot(self.G_losses, label=\"G\", color=\"red\")\n",
        "                plt.plot(self.D_losses, label=\"D\", color=\"blue\")\n",
        "            #max_y = max([max(self.D_losses),max(self.G_losses)])\n",
        "            #for i in range(1,_num_epochs):# for epoche\n",
        "            #    plt.vlines(len(self.dataloader)*i,0,max_y,colors=\"gray\",linestyles=\"dotted\",alpha=0.3)\n",
        "            plt.xlabel(\"iterations\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.legend()\n",
        "            #plt.xticks(range(0, num_epochs))\n",
        "            plt.show()\n",
        "        else: raise Exception(\"The model has to be trained before plotting the losses\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  gan1 = GAN(type='DCGAN')\n",
        "  gan1.train()\n",
        "  gan1.save(saveDiscriminator=True)\n",
        "  gan1.plot_generated_images()\n",
        "  gan1.plot_training_loss()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "LSGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}